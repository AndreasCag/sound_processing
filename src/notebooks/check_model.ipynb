{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76cd3c6-9f52-44c5-96b4-42786a494d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cc42e2-b594-4791-b45c-05f8982d6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path2add = '/Users/a.anikin/repos/cv_court_lines_detection/sound_processing/src'\n",
    "if (not (path2add in sys.path)) :\n",
    "    sys.path.append(path2add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e53e10a-c0fc-40f4-9fb7-50c01a7e0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import BreathingDataset\n",
    "from model import RawAudioCNN\n",
    "from paper_model import m5, m18\n",
    "from train import train_model\n",
    "from augmentations import train_augment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a43f1312-522b-42ea-b887-73d09a1fa5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_data = RawAudioCNN.from_checkpoint('/Users/a.anikin/repos/cv_court_lines_detection/sound_processing/models/best_model_3_0.994.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82cad23d-791b-424d-802a-259c826e7012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RawAudioCNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): AdaptiveAvgPool1d(output_size=8)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint_data['model']\n",
    "checkpoint_data['model'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0cde75-b106-41e9-9ba0-08ef4271f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb77997-c805-4eb0-b99d-45abf3a836c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c2d901ca-a174-4c2a-ac9d-100a3bd4c082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124.4589375"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# waveform, rate = torchaudio.load('/Users/a.anikin/repos/cv_court_lines_detection/sound_processing/data/test/Voice_breath3.wav')\n",
    "waveform, rate = torchaudio.load('/Users/a.anikin/repos/cv_court_lines_detection/sound_processing/data/test/Voice_noise3.wav')\n",
    "\n",
    "waveform.shape[1] / rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "33d34041-2fc9-47df-b36f-6325211da923",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "77898402-8493-4790-ac88-5df23ca632e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampler = torchaudio.transforms.Resample(rate, model_rate)\n",
    "waveform = resampler(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "05438964-e763-47a8-9bed-2ae93776a1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124.4589375"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform.shape[1] / model_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "630994dd-b2f8-455d-a73a-293bd02ec838",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_audio_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bafec78c-0b00-42ce-a884-90ff8749d810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1991343])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d65ccd29-071f-4d58-b501-b3b608bf1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1e873123-e8d5-43e4-9d19-276d4cb6688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64000])\n",
      "For sec 0 to 4: -3.124312400817871\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 1 to 5: -1.0468149185180664\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 2 to 6: -0.135947585105896\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 3 to 7: -1.704065203666687\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 4 to 8: -0.21284395456314087\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 5 to 9: -0.0356719046831131\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 6 to 10: -0.8093961477279663\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 7 to 11: -1.488232970237732\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 8 to 12: -1.3703142404556274\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 9 to 13: -0.9190977215766907\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 10 to 14: 1.4164097309112549\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 11 to 15: 0.8753427863121033\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 12 to 16: 1.0296931266784668\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 13 to 17: 0.5077635049819946\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 14 to 18: 0.930741548538208\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 15 to 19: 1.6288776397705078\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 16 to 20: 0.92246013879776\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 17 to 21: 0.41366106271743774\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 18 to 22: -3.1678383350372314\n",
      "torch.Size([1, 1, 64000])\n",
      "For sec 19 to 23: -5.489725589752197\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    waveform_part = waveform[:, i * model_rate: (i + model_audio_size) * model_rate].unsqueeze(0)\n",
    "    print(waveform_part.shape)\n",
    "\n",
    "    print(f\"For sec {i} to {i + model_audio_size}: {m18(waveform_part)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419986d-beed-4c8b-97b0-b535a088e2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b944d601-864e-4b8b-b256-dbd29fd71186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f620f01-7aa9-43ea-8004-49381e648ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataset import BreathingDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2fca055e-2bdf-4755-9f6c-8fc3c76c0752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Voice_breath_denoised - torch.Size([1, 2991840])\n",
      "Loaded 08_male_21_TLong_denoised - torch.Size([1, 17389440])\n",
      "Loaded 15_female_21_PPhuong_denoised - torch.Size([1, 17170560])\n",
      "Loaded 06_male_21_QViet_denoised - torch.Size([1, 17472960])\n",
      "Loaded 16_male_21_TTung_denoised - torch.Size([1, 17219520])\n",
      "Loaded 22_male_21_VHung_denoised - torch.Size([1, 17454240])\n",
      "Loaded 14_male_21_Khanh_denoised - torch.Size([1, 16797120])\n",
      "Loaded 20_male_21_Viet_denoised - torch.Size([1, 17274240])\n",
      "Loaded 18_male_21_Hoa_denoised - torch.Size([1, 17433120])\n",
      "Loaded 03_male_21_BDuong_denoised - torch.Size([1, 17301600])\n",
      "Loaded 29_male_19_Cong_denoised - torch.Size([1, 17426400])\n",
      "Loaded 23_male_21_CNDuong_denoised - torch.Size([1, 17425920])\n",
      "Loaded 17_male_21_Trung_denoised - torch.Size([1, 17391840])\n",
      "Loaded 10_male_21_Nam_denoised - torch.Size([1, 17479680])\n",
      "Loaded 24_female_21_MPham_denoised - torch.Size([1, 17452320])\n",
      "Loaded 04_female_21_LAnh_denoised - torch.Size([1, 15640800])\n",
      "Loaded 19_male_21_Minh_denoised - torch.Size([1, 17043360])\n",
      "Loaded 05_male_21_NLinh_denoised - torch.Size([1, 17377920])\n",
      "Loaded 11_female_21_Tam_denoised - torch.Size([1, 17463360])\n",
      "Loaded 28_male_19_VHoa_asthma_denoised - torch.Size([1, 17531520])\n",
      "Loaded 21_male_21_Hai_denoised - torch.Size([1, 17431200])\n",
      "Loaded 07_male_21_MQuang_denoised - torch.Size([1, 16915680])\n",
      "Loaded 27_female_19_TThanh_denoised - torch.Size([1, 16391040])\n",
      "Loaded random audios: 6\n",
      "Loaded Voice_breath2_denoised - torch.Size([1, 2962560])\n",
      "Loaded random audios: 2\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"/Users/a.anikin/repos/cv_court_lines_detection/sound_processing/data/Breath-Data\")\n",
    "noise_dir = Path(\"/Users/a.anikin/repos/cv_court_lines_detection/sound_processing/data/noise\")\n",
    "noise_val_dir = Path(\"/Users/a.anikin/repos/cv_court_lines_detection/sound_processing/data/noise_val\")\n",
    "\n",
    "sounds_list = [\n",
    "    data_dir / \"Voice_breath_denoised.mp3\",\n",
    "    data_dir / \"08_male_21_TLong_denoised.mp3\",\n",
    "    data_dir / \"15_female_21_PPhuong_denoised.mp3\",\n",
    "    data_dir / \"06_male_21_QViet_denoised.mp3\",\n",
    "    data_dir / \"16_male_21_TTung_denoised.mp3\",\n",
    "    data_dir / \"22_male_21_VHung_denoised.mp3\",\n",
    "    data_dir / \"14_male_21_Khanh_denoised.mp3\",\n",
    "    data_dir / \"20_male_21_Viet_denoised.mp3\",\n",
    "    data_dir / \"18_male_21_Hoa_denoised.mp3\",\n",
    "    data_dir / \"03_male_21_BDuong_denoised.mp3\",\n",
    "    data_dir / \"29_male_19_Cong_denoised.mp3\",\n",
    "    data_dir / \"23_male_21_CNDuong_denoised.mp3\",\n",
    "    data_dir / \"17_male_21_Trung_denoised.mp3\",\n",
    "    data_dir / \"10_male_21_Nam_denoised.mp3\",\n",
    "    data_dir / \"24_female_21_MPham_denoised.mp3\",\n",
    "    data_dir / \"04_female_21_LAnh_denoised.mp3\",\n",
    "    data_dir / \"19_male_21_Minh_denoised.mp3\",\n",
    "    data_dir / \"05_male_21_NLinh_denoised.mp3\",\n",
    "    data_dir / \"11_female_21_Tam_denoised.mp3\",\n",
    "    data_dir / \"28_male_19_VHoa_asthma_denoised.mp3\",\n",
    "    data_dir / \"21_male_21_Hai_denoised.mp3\",\n",
    "    data_dir / \"07_male_21_MQuang_denoised.mp3\",\n",
    "    data_dir / \"27_female_19_TThanh_denoised.mp3\",\n",
    "    data_dir / \"01_male_23_BQuyen_denoised.mp3\",\n",
    "    data_dir / \"09_male_21_Ngon_denoised.mp3\",\n",
    "    data_dir / \"12_male_21_Tam_denoised.mp3\",\n",
    "    data_dir / \"13_female_20_TNhi_denoised.mp3\",\n",
    "    data_dir / \"02_male_22_PTuan_denoised.mp3\",\n",
    "    data_dir / \"26_female_19_Linh_denoised.mp3\",\n",
    "    data_dir / \"Voice_breath2_denoised.mp3\",\n",
    "]\n",
    "    \n",
    "# Split into train / validation\n",
    "train_list = sounds_list[:-7]\n",
    "val_list   = [data_dir / \"Voice_breath2_denoised.mp3\"]\n",
    "# Create datasets (e.g., 1 second max length => 16000 samples)\n",
    "train_dataset = BreathingDataset(train_list, samples_amount=1000, random_audios_folder=noise_dir)\n",
    "val_dataset = BreathingDataset(val_list, samples_amount=1000, random_audios_folder=noise_val_dir, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a1e7dc2-32b6-4e93-a453-a8451476e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val item 0 - 0\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: tensor([-32.5396], grad_fn=<SqueezeBackward1>)\n",
      "Val item 1 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: tensor([0.2056], grad_fn=<SqueezeBackward1>)\n",
      "Val item 2 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: tensor([0.2072], grad_fn=<SqueezeBackward1>)\n",
      "Val item 3 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: tensor([0.2256], grad_fn=<SqueezeBackward1>)\n",
      "Val item 4 - 0\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: tensor([-0.7811], grad_fn=<SqueezeBackward1>)\n",
      "Val item 5 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: tensor([0.1921], grad_fn=<SqueezeBackward1>)\n",
      "Val item 6 - 0\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: tensor([-8.6989], grad_fn=<SqueezeBackward1>)\n",
      "Val item 7 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: tensor([0.2115], grad_fn=<SqueezeBackward1>)\n",
      "Val item 8 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: tensor([0.1928], grad_fn=<SqueezeBackward1>)\n",
      "Val item 9 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: tensor([0.2289], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    val_item = val_dataset[i]\n",
    "    print(f\"Val item {i} - {val_item[1]}\")\n",
    "    print(val_item[0].unsqueeze(0).unsqueeze(0).shape)\n",
    "    result = checkpoint_data['model'](val_item[0].unsqueeze(0).unsqueeze(0))\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2883566-cda8-45dd-9999-e09994f55ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0b372adc-136a-44a3-8985-cc0a594dead5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_blocks): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(1, 64, kernel_size=(80,), stride=(4,), padding=(38,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (10): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pool_blocks): ModuleList(\n",
       "    (0-2): 3 x MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): MaxPool1d(kernel_size=4, stride=4, padding=2, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (linear): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load('/Users/a.anikin/repos/cv_court_lines_detection/sound_processing/models/best_model_38_0.934.pth', map_location='cpu')\n",
    "\n",
    "# # Create new model instance\n",
    "# model = cls()\n",
    "m18.load_state_dict(checkpoint['model_state_dict'])\n",
    "m18.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f8dbf23-a957-4f67-8774-67ee09deb6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Voice_breath - torch.Size([1, 997158])\n",
      "Loaded Voice_breath4 - torch.Size([1, 1795553])\n",
      "Loaded Voice_breath5 - torch.Size([1, 2596608])\n",
      "Loaded Voice_breath6 - torch.Size([1, 949232])\n",
      "Loaded Voice_breath7 - torch.Size([1, 3436416])\n",
      "Loaded Voice_breath8 - torch.Size([1, 1046198])\n",
      "Loaded Voice_breath9 - torch.Size([1, 3220992])\n",
      "Loaded random audios: 12\n",
      "Loaded Voice_breath2 - torch.Size([1, 987498])\n",
      "Loaded Voice_breath10 - torch.Size([1, 1020192])\n",
      "Loaded Voice_breath11 - torch.Size([1, 3104640])\n",
      "Loaded random audios: 4\n"
     ]
    }
   ],
   "source": [
    "sounds_list = [\n",
    "    data_dir / \"Voice_breath.mp3\",\n",
    "    data_dir / \"Voice_breath4.mp3\",\n",
    "    data_dir / \"Voice_breath5.mp3\",\n",
    "    data_dir / \"Voice_breath6.mp3\",\n",
    "    data_dir / \"Voice_breath7.mp3\",\n",
    "    data_dir / \"Voice_breath8.mp3\",\n",
    "    data_dir / \"Voice_breath9.mp3\",\n",
    "    # data_dir / \"08_male_21_TLong_denoised.mp3\",\n",
    "    # data_dir / \"15_female_21_PPhuong_denoised.mp3\",\n",
    "    # data_dir / \"06_male_21_QViet_denoised.mp3\",\n",
    "    # data_dir / \"16_male_21_TTung_denoised.mp3\",\n",
    "    # data_dir / \"22_male_21_VHung_denoised.mp3\",\n",
    "    # data_dir / \"14_male_21_Khanh_denoised.mp3\",\n",
    "    # data_dir / \"20_male_21_Viet_denoised.mp3\",\n",
    "    # data_dir / \"18_male_21_Hoa_denoised.mp3\",\n",
    "    # data_dir / \"03_male_21_BDuong_denoised.mp3\",\n",
    "    # data_dir / \"29_male_19_Cong_denoised.mp3\",\n",
    "    # data_dir / \"23_male_21_CNDuong_denoised.mp3\",\n",
    "    # data_dir / \"17_male_21_Trung_denoised.mp3\",\n",
    "    # data_dir / \"10_male_21_Nam_denoised.mp3\",\n",
    "    # data_dir / \"24_female_21_MPham_denoised.mp3\",\n",
    "    # data_dir / \"04_female_21_LAnh_denoised.mp3\",\n",
    "    # data_dir / \"19_male_21_Minh_denoised.mp3\",\n",
    "    # data_dir / \"05_male_21_NLinh_denoised.mp3\",\n",
    "    # data_dir / \"11_female_21_Tam_denoised.mp3\",\n",
    "    # data_dir / \"28_male_19_VHoa_asthma_denoised.mp3\",\n",
    "    # data_dir / \"21_male_21_Hai_denoised.mp3\",\n",
    "    # data_dir / \"07_male_21_MQuang_denoised.mp3\",\n",
    "    # data_dir / \"27_female_19_TThanh_denoised.mp3\",\n",
    "    # data_dir / \"01_male_23_BQuyen_denoised.mp3\",\n",
    "    # data_dir / \"09_male_21_Ngon_denoised.mp3\",\n",
    "    # data_dir / \"12_male_21_Tam_denoised.mp3\",\n",
    "    # data_dir / \"13_female_20_TNhi_denoised.mp3\",\n",
    "    # data_dir / \"02_male_22_PTuan_denoised.mp3\",\n",
    "    # data_dir / \"26_female_19_Linh_denoised.mp3\",\n",
    "    data_dir / \"Voice_breath2.mp3\",\n",
    "    data_dir / \"Voice_breath10.mp3\",\n",
    "    data_dir / \"Voice_breath11.mp3\",\n",
    "]\n",
    "    \n",
    "# Split into train / validation\n",
    "train_list = sounds_list[:-3]\n",
    "val_list   = sounds_list[-3:]\n",
    "# Create datasets (e.g., 1 second max length => 16000 samples)\n",
    "train_dataset = BreathingDataset(train_list, samples_amount=1000, random_audios_folder=noise_dir)\n",
    "val_dataset = BreathingDataset(val_list, samples_amount=1000, random_audios_folder=noise_val_dir, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "68398718-63ab-4224-9f34-a7c7eab9ee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val item 0 - 0\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: -0.03221440687775612\n",
      "Val item 1 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: 0.08220379799604416\n",
      "Val item 2 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: 0.2917770445346832\n",
      "Val item 3 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: 3.9346067905426025\n",
      "Val item 4 - 0\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: -1.2589125633239746\n",
      "Val item 5 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: 1.7654154300689697\n",
      "Val item 6 - 0\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: -0.038218021392822266\n",
      "Val item 7 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: 3.854412078857422\n",
      "Val item 8 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: 4.396386623382568\n",
      "Val item 9 - 1\n",
      "torch.Size([1, 1, 64000])\n",
      "Result: 0.2698443531990051\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    val_item = val_dataset[i]\n",
    "    print(f\"Val item {i} - {val_item[1]}\")\n",
    "    print(val_item[0].unsqueeze(0).unsqueeze(0).shape)\n",
    "    result = m18(val_item[0].unsqueeze(0).unsqueeze(0))\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8879586c-60ed-4036-8761-32e86add09e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.17.0.tar.gz (12.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "  Idone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?2done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?2done\n",
      "\u001b[?25hCollecting onnxscript\n",
      "  Downloading onnxscript-0.2.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/homebrew/lib/python3.13/site-packages (from onnx) (2.2.2)\n",
      "Collecting protobuf>=3.20.2 (from onnx)\n",
      "  Using cached protobuf-5.29.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages (from onnxscript) (4.12.2)\n",
      "Collecting ml_dtypes (from onnxscript)\n",
      "  Downloading ml_dtypes-0.5.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages (from onnxscript) (24.2)\n",
      "Downloading onnxscript-0.2.0-py3-none-any.whl (691 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m691.6/691.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-5.29.3-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading ml_dtypes-0.5.1-cp313-cp313-macosx_10_13_universal2.whl (667 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m667.7/667.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Building wheels for collected packages: onnx\n",
      "done\n",
      "\u001b[?25h  Created wheel for onnx: filename=onnx-1.17.0-cp313-cp313-macosx_14_0_arm64.whl size=14903212 sha256=a2a3feec89399addd4dbed54354481261c98f611fc5359b2b4b7d59cf8849544\n",
      "  Stored in directory: /Users/a.anikin/Library/Caches/pip/wheels/ac/c1/73/4fa9bcde707160b22aa12f1d86d447ee5de075399d45f566e9\n",
      "Successfully built onnx\n",
      "Installing collected packages: protobuf, ml_dtypes, onnx, onnxscript\n",
      "Successfully installed ml_dtypes-0.5.1 onnx-1.17.0 onnxscript-0.2.0 protobuf-5.29.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install onnx onnxscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5184ebdb-0502-4143-9a13-b9d64e9c6a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/torch/onnx/_internal/_exporter_legacy.py:101: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/torch/onnx/_internal/fx/passes/readability.py:52: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  new_node = self.module.graph.get_attr(normalized_name)\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/torch/fx/graph.py:1801: UserWarning: Node conv_blocks_0_1_running_mean target conv_blocks/0/1/running_mean conv_blocks/0/1/running_mean of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/torch/fx/graph.py:1801: UserWarning: Node conv_blocks_0_1_running_var target conv_blocks/0/1/running_var conv_blocks/0/1/running_var of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/torch/fx/graph.py:1801: UserWarning: Node conv_blocks_1_1_running_mean target conv_blocks/1/1/running_mean conv_blocks/1/1/running_mean of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/torch/fx/graph.py:1801: UserWarning: Node conv_blocks_1_1_running_var target conv_blocks/1/1/running_var conv_blocks/1/1/running_var of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/torch/fx/graph.py:1801: UserWarning: Node conv_blocks_1_4_running_mean target conv_blocks/1/4/running_mean conv_blocks/1/4/running_mean of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/torch/fx/graph.py:1810: UserWarning: Additional 29 warnings suppressed about get_attr references\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/torch/onnx/_internal/fx/onnxfunction_dispatcher.py:503: FutureWarning: 'onnxscript.values.TracedOnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  self.param_schema = self.onnxfunction.param_schemas()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 1 of general pattern rewrite rules.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch_input = torch.randn(1, 1, 64000)\n",
    "onnx_program = torch.onnx.dynamo_export(m18, torch_input)\n",
    "onnx_program.save(\"my_audio_classifier.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b150b1a-8b2d-4d93-9ec0-676f9eee922f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
